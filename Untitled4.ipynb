{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "numpy.random.seed(999)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Reshape,Lambda,LSTM,merge,normalization,BatchNormalization,Merge\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "from keras.layers import Input, Concatenate\n",
    "from keras.models import Model\n",
    "from keras.engine import Layer\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.optimizers import Adam,RMSprop\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Reshape,Lambda,Embedding,Input,Dropout,ZeroPadding2D, Bidirectional\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "\n",
    "from keras.engine import Layer\n",
    "from keras import initializers\n",
    "from keras.optimizers import Adam,RMSprop\n",
    "import h5py\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Flatten, Dense, Dropout\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.optimizers import SGD\n",
    "#import cv2\n",
    "import numpy as np\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.callbacks import EarlyStopping,ReduceLROnPlateau\n",
    "from pprint import pprint\n",
    "from keras.layers import Embedding\n",
    "import importlib\n",
    "from kraino.utils import data_provider\n",
    "importlib.reload(data_provider)\n",
    "from kraino.utils import data_provider\n",
    "from keras.layers import Add, Multiply\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.image as mpimg\n",
    "from matplotlib import pyplot as plt\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extract data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['x', 'y', 'img_name', 'img_ind', 'question_id', 'end_of_question', 'end_of_answer', 'answer_words_delimiter'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp = data_provider.select['daquar-triples']\n",
    "\n",
    "train_text_representation = dp['text'](train_or_test='train')\n",
    "train_text_representation.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "words frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 9847), ('?', 6795), ('what', 5847), ('is', 5368), ('on', 2909)]\n",
      "[('gery', 1), ('image10', 1), ('locker', 1), ('conference', 1), ('been', 1)]\n"
     ]
    }
   ],
   "source": [
    "from toolz import frequencies\n",
    "train_raw_x = train_text_representation['x']\n",
    "# we start from building the frequencies table\n",
    "wordcount_x = frequencies(' '.join(train_raw_x).split(' '))\n",
    "# print the most and least frequent words\n",
    "n_show = 5\n",
    "print(sorted(wordcount_x.items(), key=lambda x: x[1], reverse=True)[:n_show])\n",
    "print(sorted(wordcount_x.items(), key=lambda x: x[1])[:n_show])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kraino.utils.input_output_space import build_vocabulary\n",
    "\n",
    "# This function takes wordcounts and returns word2index - mapping from words into indices, \n",
    "# and index2word - mapping from indices to words.\n",
    "word2index_x, index2word_x = build_vocabulary(\n",
    "    this_wordcount=wordcount_x,\n",
    "    truncate_to_most_frequent=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['what is on the right side of the black telephone and on the left side of the red chair ?', 'what is in front of the white door on the left side of the desk ?', 'what is on the desk ?']\n",
      "[[4, 5, 6, 7, 8, 9, 10, 7, 11, 12, 13, 6, 7, 14, 9, 10, 7, 15, 16, 17, 3], [4, 5, 18, 19, 10, 7, 20, 21, 6, 7, 14, 9, 10, 7, 22, 17, 3], [4, 5, 6, 7, 22, 17, 3]]\n"
     ]
    }
   ],
   "source": [
    "from kraino.utils.input_output_space import encode_questions_index\n",
    "one_hot_x = encode_questions_index(train_raw_x, word2index_x)\n",
    "print(train_raw_x[:3])\n",
    "print(one_hot_x[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6795"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_raw_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  4,  5,  6,  7,  8,  9, 10,\n",
       "         7, 11, 12, 13,  6,  7, 14,  9, 10,  7, 15, 16, 17,  3],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  4,  5, 18,\n",
       "        19, 10,  7, 20, 21,  6,  7, 14,  9, 10,  7, 22, 17,  3],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  4,  5,  6,  7, 22, 17,  3]],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "MAXLEN=30\n",
    "train_x = sequence.pad_sequences(one_hot_x, maxlen=MAXLEN)\n",
    "train_x[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6795, 503)\n",
      "(6795, 30)\n",
      "(6795, 503)\n"
     ]
    }
   ],
   "source": [
    "MAX_ANSWER_TIME_STEPS=1\n",
    "\n",
    "from kraino.utils.input_output_space import encode_answers_one_hot\n",
    "train_raw_y = train_text_representation['y']\n",
    "wordcount_y = frequencies(' '.join(train_raw_y).replace(', ',' ').split(' '))\n",
    "word2index_y, index2word_y = build_vocabulary(this_wordcount=wordcount_y)\n",
    "train_y, _ = encode_answers_one_hot(\n",
    "    train_raw_y, \n",
    "    word2index_y, \n",
    "    answer_words_delimiter=train_text_representation['answer_words_delimiter'],\n",
    "    is_only_first_answer_word=True,\n",
    "    max_answer_time_steps=MAX_ANSWER_TIME_STEPS)\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "503"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2index_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['what is on the left side of the white oven on the floor and on right side of '\n",
      " 'the blue armchair ?',\n",
      " 'what is on the left side of the fire extinguisher and on the right side of '\n",
      " 'the chair ?',\n",
      " 'what is between the the two white and black garbage bins ?']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0,   0,   4,   5,   6,   7,  14,   9,\n",
       "         10,   7,  20, 188,   6,   7,  67,  13,   6,   8,   9,  10,   7,\n",
       "         44, 227,  17,   3],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   4,   5,   6,\n",
       "          7,  14,   9,  10,   7,  61,  62,  13,   6,   7,   8,   9,  10,\n",
       "          7,  16,  17,   3],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   4,   5,  52,   7,   7,  92,  20,  13,  11,\n",
       "         59, 249,  17,   3]], dtype=int32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_text_representation = dp['text'](train_or_test='test')\n",
    "test_raw_x = test_text_representation['x']\n",
    "test_one_hot_x = encode_questions_index(test_raw_x, word2index_x)\n",
    "test_x = sequence.pad_sequences(test_one_hot_x, maxlen=MAXLEN)\n",
    "pprint(test_raw_x[:3])\n",
    "test_x[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5673, 503)\n"
     ]
    }
   ],
   "source": [
    "test_raw_y = test_text_representation['y']\n",
    "\n",
    "wordcount_y = frequencies(' '.join(test_raw_y).replace(', ',' ').split(' '))\n",
    "#word2index_y, index2word_y = build_vocabulary(this_wordcount=wordcount_y)\n",
    "test_y, _ = encode_answers_one_hot(\n",
    "    test_raw_y, \n",
    "    word2index_y, \n",
    "    answer_words_delimiter=train_text_representation['answer_words_delimiter'],\n",
    "    is_only_first_answer_word=True,\n",
    "    max_answer_time_steps=MAX_ANSWER_TIME_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "503"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2index_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the data loaded\n",
      "no memories\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(6795, 1000)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_image_names = train_text_representation['img_name']\n",
    "# the name for visual features that we use\n",
    "# CNN_NAME='vgg_net'\n",
    "# CNN_NAME='googlenet'\n",
    "CNN_NAME='googlenet'\n",
    "# the layer in CNN that is used to extract features\n",
    "# PERCEPTION_LAYER='fc7'\n",
    "# PERCEPTION_LAYER='pool5-7x7_s1'\n",
    "# PERCEPTION_LAYER='res5c-152'\n",
    "PERCEPTION_LAYER='loss3-classifier' # l2 prefix since there are l2-normalized visual features\n",
    "\n",
    "train_visual_features = dp['perception'](\n",
    "    train_or_test='train',\n",
    "    names_list=train_image_names,\n",
    "    parts_extractor=None,\n",
    "    max_parts=None,\n",
    "    perception=CNN_NAME,\n",
    "    layer=PERCEPTION_LAYER,\n",
    "    second_layer=None\n",
    "    )\n",
    "train_visual_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the data loaded\n",
      "no memories\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5673, 1000)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_image_names = test_text_representation['img_name']\n",
    "test_visual_features = dp['perception'](\n",
    "    train_or_test='test',\n",
    "    names_list=test_image_names,\n",
    "    parts_extractor=None,\n",
    "    max_parts=None,\n",
    "    perception=CNN_NAME,\n",
    "    layer=PERCEPTION_LAYER,\n",
    "    second_layer=None\n",
    "    )\n",
    "test_visual_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6795, 503)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kawhi/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:47: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/kawhi/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:349: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/kawhi/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3147: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/kawhi/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1044: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/kawhi/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:2683: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/kawhi/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:675: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/kawhi/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:2554: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/kawhi/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/kawhi/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:766: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/kawhi/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:519: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Train on 6795 samples, validate on 5673 samples\n",
      "Epoch 1/40\n",
      "6795/6795 [==============================] - 4s - loss: 6.6504 - acc: 0.0238 - val_loss: 6.2513 - val_acc: 0.0189\n",
      "Epoch 2/40\n",
      "6795/6795 [==============================] - 0s - loss: 5.2308 - acc: 0.1298 - val_loss: 5.6826 - val_acc: 0.0478\n",
      "Epoch 3/40\n",
      "6795/6795 [==============================] - 0s - loss: 4.3658 - acc: 0.1959 - val_loss: 5.3519 - val_acc: 0.0585\n",
      "Epoch 4/40\n",
      "6795/6795 [==============================] - 0s - loss: 3.8308 - acc: 0.2397 - val_loss: 5.1589 - val_acc: 0.0767\n",
      "Epoch 5/40\n",
      "6795/6795 [==============================] - 0s - loss: 3.4445 - acc: 0.2773 - val_loss: 5.1728 - val_acc: 0.0917\n",
      "Epoch 6/40\n",
      "6795/6795 [==============================] - 0s - loss: 3.1453 - acc: 0.3232 - val_loss: 5.1996 - val_acc: 0.1014\n",
      "Epoch 7/40\n",
      "6795/6795 [==============================] - 0s - loss: 2.8728 - acc: 0.3597 - val_loss: 5.1688 - val_acc: 0.1435\n",
      "Epoch 8/40\n",
      "6795/6795 [==============================] - 0s - loss: 2.6503 - acc: 0.3965 - val_loss: 5.2525 - val_acc: 0.1424\n",
      "Epoch 9/40\n",
      "6795/6795 [==============================] - 0s - loss: 2.4640 - acc: 0.4243 - val_loss: 5.1706 - val_acc: 0.1606\n",
      "Epoch 10/40\n",
      "6795/6795 [==============================] - 0s - loss: 2.3101 - acc: 0.4527 - val_loss: 5.1253 - val_acc: 0.1588\n",
      "Epoch 11/40\n",
      "6795/6795 [==============================] - 0s - loss: 2.1462 - acc: 0.4821 - val_loss: 5.0911 - val_acc: 0.1553\n",
      "Epoch 12/40\n",
      "6795/6795 [==============================] - 0s - loss: 2.0227 - acc: 0.5054 - val_loss: 5.1374 - val_acc: 0.1315\n",
      "Epoch 13/40\n",
      "6795/6795 [==============================] - 0s - loss: 1.8896 - acc: 0.5361 - val_loss: 5.0258 - val_acc: 0.1366\n",
      "Epoch 14/40\n",
      "6795/6795 [==============================] - 0s - loss: 1.7812 - acc: 0.5558 - val_loss: 4.8857 - val_acc: 0.1632\n",
      "Epoch 15/40\n",
      "6795/6795 [==============================] - 0s - loss: 1.6963 - acc: 0.5611 - val_loss: 4.7927 - val_acc: 0.1456\n",
      "Epoch 16/40\n",
      "6795/6795 [==============================] - 0s - loss: 1.5994 - acc: 0.5772 - val_loss: 4.7461 - val_acc: 0.1363\n",
      "Epoch 17/40\n",
      "6795/6795 [==============================] - 0s - loss: 1.5226 - acc: 0.5956 - val_loss: 4.7053 - val_acc: 0.1440\n",
      "Epoch 18/40\n",
      "6795/6795 [==============================] - 0s - loss: 1.4562 - acc: 0.6155 - val_loss: 4.6063 - val_acc: 0.1602\n",
      "Epoch 19/40\n",
      "6795/6795 [==============================] - 0s - loss: 1.3814 - acc: 0.6312 - val_loss: 4.5426 - val_acc: 0.1518\n",
      "Epoch 20/40\n",
      "6795/6795 [==============================] - 0s - loss: 1.3491 - acc: 0.6393 - val_loss: 4.5148 - val_acc: 0.1705\n",
      "Epoch 21/40\n",
      "6795/6795 [==============================] - 0s - loss: 1.3013 - acc: 0.6483 - val_loss: 4.5749 - val_acc: 0.1539\n",
      "Epoch 22/40\n",
      "6795/6795 [==============================] - 0s - loss: 1.2355 - acc: 0.6648 - val_loss: 4.5099 - val_acc: 0.1638\n",
      "Epoch 23/40\n",
      "6795/6795 [==============================] - 0s - loss: 1.1739 - acc: 0.6780 - val_loss: 4.3445 - val_acc: 0.1713\n",
      "Epoch 24/40\n",
      "6795/6795 [==============================] - 0s - loss: 1.1429 - acc: 0.6867 - val_loss: 4.4806 - val_acc: 0.1872\n",
      "Epoch 25/40\n",
      "6795/6795 [==============================] - 0s - loss: 1.1134 - acc: 0.6892 - val_loss: 4.5114 - val_acc: 0.1683\n",
      "Epoch 26/40\n",
      "6795/6795 [==============================] - 0s - loss: 1.0618 - acc: 0.7023 - val_loss: 4.3309 - val_acc: 0.1726\n",
      "Epoch 27/40\n",
      "6795/6795 [==============================] - 0s - loss: 1.0472 - acc: 0.7048 - val_loss: 4.5872 - val_acc: 0.1572\n",
      "Epoch 28/40\n",
      "6795/6795 [==============================] - 0s - loss: 0.9997 - acc: 0.7227 - val_loss: 4.4341 - val_acc: 0.1816\n",
      "Epoch 29/40\n",
      "6795/6795 [==============================] - 0s - loss: 0.9784 - acc: 0.7201 - val_loss: 4.6007 - val_acc: 0.1668\n",
      "Epoch 30/40\n",
      "6795/6795 [==============================] - 0s - loss: 0.9538 - acc: 0.7302 - val_loss: 4.6416 - val_acc: 0.1761\n",
      "Epoch 31/40\n",
      "6795/6795 [==============================] - 0s - loss: 0.9482 - acc: 0.7342 - val_loss: 4.6798 - val_acc: 0.1632\n",
      "Epoch 32/40\n",
      "6795/6795 [==============================] - 0s - loss: 0.8946 - acc: 0.7461 - val_loss: 4.9477 - val_acc: 0.1682\n",
      "Epoch 33/40\n",
      "6795/6795 [==============================] - 0s - loss: 0.9086 - acc: 0.7313 - val_loss: 4.8842 - val_acc: 0.1747\n",
      "Epoch 34/40\n",
      "6795/6795 [==============================] - 0s - loss: 0.8806 - acc: 0.7473 - val_loss: 5.5348 - val_acc: 0.1454\n",
      "Epoch 35/40\n",
      "6795/6795 [==============================] - 0s - loss: 0.8719 - acc: 0.7494 - val_loss: 5.1436 - val_acc: 0.1703\n",
      "Epoch 36/40\n",
      "6795/6795 [==============================] - 0s - loss: 0.8184 - acc: 0.7595 - val_loss: 5.2194 - val_acc: 0.1837\n",
      "Epoch 37/40\n",
      "6795/6795 [==============================] - 0s - loss: 0.8130 - acc: 0.7635 - val_loss: 5.4688 - val_acc: 0.1625\n",
      "Epoch 38/40\n",
      "6795/6795 [==============================] - 0s - loss: 0.7758 - acc: 0.7770 - val_loss: 5.9212 - val_acc: 0.1495\n",
      "Epoch 39/40\n",
      "6795/6795 [==============================] - 0s - loss: 0.7744 - acc: 0.7754 - val_loss: 5.8452 - val_acc: 0.1532\n",
      "Epoch 40/40\n",
      "6795/6795 [==============================] - 0s - loss: 0.7268 - acc: 0.7879 - val_loss: 5.7880 - val_acc: 0.1669\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbfa9d736a0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import merge\n",
    "## I\n",
    "inp = Input(shape=(1000,))\n",
    "I = Dense(100)(inp)\n",
    "\n",
    "## Q\n",
    "sequence_q = Input(shape=(30,))\n",
    "embedded_sequences = Embedding(len(word2index_x) ,100)(sequence_q)\n",
    "\n",
    "time_distributed_merge_layer = Lambda(function=lambda x: K.sum(x, axis=1),output_shape=(100,))(embedded_sequences)\n",
    "x = Dense(128)(time_distributed_merge_layer)\n",
    "x = BatchNormalization()(x)\n",
    "Q = Dense(100)(x)\n",
    "Q = BatchNormalization()(Q)\n",
    "\n",
    "l = Multiply()([I,Q])\n",
    "l = Dropout(.25)(l)\n",
    "l = Dense(503, activation='softmax')(l)\n",
    "\n",
    "model = Model([inp,sequence_q],l)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])\n",
    "model.fit([train_visual_features,train_x],train_y,epochs=40,batch_size=512,validation_data=([test_visual_features,test_x], test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.topology.InputLayer object at 0x7f814b465c50>\n",
      "<keras.layers.embeddings.Embedding object at 0x7f814b200c18>\n",
      "<keras.layers.core.Lambda object at 0x7f814b200d68>\n",
      "<keras.layers.core.Dense object at 0x7f814b205c50>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f814b205fd0>\n",
      "<keras.engine.topology.InputLayer object at 0x7f814b204278>\n",
      "<keras.layers.core.Dense object at 0x7f814b206630>\n",
      "<keras.layers.core.Dense object at 0x7f814b201438>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f814b207978>\n",
      "<keras.layers.merge.Multiply object at 0x7f814b215b00>\n",
      "<keras.layers.core.Dropout object at 0x7f814b2166d8>\n",
      "<keras.layers.core.Dense object at 0x7f814b218f60>\n"
     ]
    }
   ],
   "source": [
    "for i in model.layers:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kawhi/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3014: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/kawhi/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1062: calling reduce_prod_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.applications import VGG16\n",
    "import tensorflow as tf\n",
    "model2 = keras.applications.VGG16(include_top=True)\n",
    "from PIL import Image\n",
    "\n",
    "im = Image.open(\"/home/kawhi/Desktop/dog-puppy-on-garden-royalty-free-image-1586966191.jpg\")\n",
    "x = im.resize((224,224))\n",
    "import numpy as np\n",
    "x = np.array(x)\n",
    "#x = x.resize((244,244,3))\n",
    "tensor = tf.keras.applications.vgg16.preprocess_input(x)\n",
    "tensor = np.expand_dims(tensor, axis=0)\n",
    "features = model2.predict([tensor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3397 samples, validate on 3398 samples\n",
      "Epoch 1/10\n",
      "3397/3397 [==============================] - 0s - loss: 6.5445 - acc: 0.0253 - val_loss: 5.9868 - val_acc: 0.0371\n",
      "Epoch 2/10\n",
      "3397/3397 [==============================] - 0s - loss: 5.1491 - acc: 0.0503 - val_loss: 5.7113 - val_acc: 0.0412\n",
      "Epoch 3/10\n",
      "3397/3397 [==============================] - 0s - loss: 4.5581 - acc: 0.0916 - val_loss: 5.5936 - val_acc: 0.0409\n",
      "Epoch 4/10\n",
      "3397/3397 [==============================] - 0s - loss: 4.2577 - acc: 0.1228 - val_loss: 5.5489 - val_acc: 0.0553\n",
      "Epoch 5/10\n",
      "3397/3397 [==============================] - 0s - loss: 4.0132 - acc: 0.1534 - val_loss: 5.5358 - val_acc: 0.0556\n",
      "Epoch 6/10\n",
      "3397/3397 [==============================] - 0s - loss: 3.7809 - acc: 0.1799 - val_loss: 5.4390 - val_acc: 0.0745\n",
      "Epoch 7/10\n",
      "3397/3397 [==============================] - 0s - loss: 3.5712 - acc: 0.2002 - val_loss: 5.3738 - val_acc: 0.0789\n",
      "Epoch 8/10\n",
      "3397/3397 [==============================] - 0s - loss: 3.3812 - acc: 0.2317 - val_loss: 5.3281 - val_acc: 0.0856\n",
      "Epoch 9/10\n",
      "3397/3397 [==============================] - 0s - loss: 3.2236 - acc: 0.2467 - val_loss: 5.3316 - val_acc: 0.0918\n",
      "Epoch 10/10\n",
      "3397/3397 [==============================] - 0s - loss: 3.0758 - acc: 0.2646 - val_loss: 5.3365 - val_acc: 0.0877\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f814af4cf98>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import merge\n",
    "## I\n",
    "inp = Input(shape=(1000,))\n",
    "I = Dense(100)(inp)\n",
    "\n",
    "## Q\n",
    "sequence_q = Input(shape=(30,))\n",
    "embedded_sequences = Embedding(len(word2index_x) ,100)(sequence_q)\n",
    "\n",
    "time_distributed_merge_layer = Lambda(function=lambda x: K.sum(x, axis=1),output_shape=(100,))(embedded_sequences)\n",
    "\n",
    "x = Dense(128)(time_distributed_merge_layer)\n",
    "Q = Dense(100)(x)\n",
    "\n",
    "\n",
    "l = Add()([I,Q])\n",
    "\n",
    "l = Dense(503, activation='softmax')(l)\n",
    "\n",
    "model = Model([inp,sequence_q],l)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])\n",
    "model.fit([train_visual_features,train_x],train_y,epochs=10,batch_size=512,validation_split=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "im.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kawhi/anaconda3/lib/python3.7/site-packages/keras/legacy/interfaces.py:86: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(units=128)`\n",
      "  '` call to the Keras 2 API: ' + signature)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5096 samples, validate on 1699 samples\n",
      "Epoch 1/50\n",
      "5096/5096 [==============================] - 5s - loss: 6.1043 - acc: 0.0220 - val_loss: 5.6335 - val_acc: 0.0377\n",
      "Epoch 2/50\n",
      "5096/5096 [==============================] - 4s - loss: 4.9496 - acc: 0.0534 - val_loss: 5.2713 - val_acc: 0.0424\n",
      "Epoch 3/50\n",
      "5096/5096 [==============================] - 4s - loss: 4.5831 - acc: 0.0720 - val_loss: 5.2763 - val_acc: 0.0518\n",
      "Epoch 4/50\n",
      "3072/5096 [=================>............] - ETA: 1s - loss: 4.3050 - acc: 0.1042"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-d43933751676>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m               \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m               metrics=['acc'])\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_visual_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1483\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1484\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1485\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1487\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m   1138\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1140\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1141\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2073\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2074\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m-> 2075\u001b[0;31m                               feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m   2076\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2077\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1354\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1339\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1341\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1427\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.layers import merge\n",
    "from keras.layers import LSTM\n",
    "## I\n",
    "inp = Input(shape=(1000,))\n",
    "I = Dense(100)(inp)\n",
    "\n",
    "## Q\n",
    "sequence_q = Input(shape=(30,))\n",
    "embedded_sequences = Embedding(len(word2index_x) ,100)(sequence_q)\n",
    "lstm = LSTM(output_dim=128)\n",
    "\n",
    "time_distributed_merge_layer = lstm(embedded_sequences)\n",
    "x = Dense(128)(time_distributed_merge_layer)\n",
    "Q = Dense(100)(x)\n",
    "\n",
    "l = Concatenate()([I,Q])\n",
    "\n",
    "l = Dense(503, activation='softmax')(l)\n",
    "\n",
    "model = Model([inp,sequence_q],l)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])\n",
    "model.fit([train_visual_features,train_x],train_y,epochs=50,batch_size=512,validation_split=.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kawhi/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3014: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/kawhi/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1062: calling reduce_prod_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.applications import VGG16\n",
    "import tensorflow as tf\n",
    "model2 = keras.applications.VGG16(include_top=True)\n",
    "from PIL import Image\n",
    "\n",
    "im = Image.open(\"/home/kawhi/Pictures/35845723-horizontal-view-of-living-space-inside-house.jpg\")\n",
    "x = im.resize((224,224))\n",
    "import numpy as np\n",
    "x = np.array(x)\n",
    "#x = x.resize((244,244,3))\n",
    "tensor = tf.keras.applications.vgg16.preprocess_input(x)\n",
    "tensor = np.expand_dims(tensor, axis=0)\n",
    "features = model2.predict([tensor])\n",
    "im.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = encode_questions_index(['How many chairs?'], word2index_x)\n",
    "one_hot_question = sequence.pad_sequences(question, maxlen=MAXLEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict([features, one_hot_question])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'brown'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_raw_y[np.argmax(prediction)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
